#!/usr/bin/env python3
"""
Webスクレイピングシステム
Generated by Nocturnal Agent with GitHub Spec Kit
Task: Task: Webスクレイピングシステムの作成の実装仕様
Generated at: 2025-09-17T17:09:16.406083
"""

import requests
import sqlite3
import json
import csv
import time
import hashlib
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from dataclasses import dataclass

@dataclass
class CollectedData:
    """収集データクラス"""
    title: str
    url: str
    content: str
    source: str
    collected_at: str = None
    hash_value: str = None
    
    def __post_init__(self):
        if not self.collected_at:
            self.collected_at = datetime.now().isoformat()
        if not self.hash_value:
            self.hash_value = hashlib.md5(f"{self.title}{self.url}".encode()).hexdigest()

class WebScraper:
    """Webスクレイピングシステム"""
    
    def __init__(self, config_file: str = "config/targets.json"):
        self.config_file = Path(config_file)
        self.db_path = "data/scraped_data.db"
        self.logger = self._setup_logging()
        self.setup_database()
        self.targets = self.load_targets()
    
    def _setup_logging(self):
        """ログシステムの設定"""
        Path("logs").mkdir(exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/scraper.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def setup_database(self):
        """データベース初期化"""
        Path(self.db_path).parent.mkdir(exist_ok=True)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS scraped_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                content TEXT,
                source TEXT,
                collected_at TEXT,
                hash_value TEXT UNIQUE
            )
        """)
        
        conn.commit()
        conn.close()
        self.logger.info("データベース初期化完了")
    
    def load_targets(self) -> List[Dict]:
        """ターゲット設定を読み込み"""
        if not self.config_file.exists():
            default_targets = [
                {"name": "Example Site", "url": "https://example.com", "selector": "article"}
            ]
            self.config_file.parent.mkdir(exist_ok=True)
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(default_targets, f, indent=2, ensure_ascii=False)
            self.logger.info(f"デフォルト設定を作成: {self.config_file}")
            return default_targets
        
        with open(self.config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def scrape_website(self, target: Dict) -> List[CollectedData]:
        """Webサイトスクレイピング"""
        data_list = []
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(target['url'], headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            elements = soup.select(target['selector'])[:10]  # 最大10アイテム
            
            for element in elements:
                title_elem = element.find(['h1', 'h2', 'h3', 'a'])
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    link_elem = element.find('a')
                    url = link_elem['href'] if link_elem else target['url']
                    
                    if not url.startswith('http'):
                        url = f"{target['url']}{url}"
                    
                    data = CollectedData(
                        title=title[:200],
                        url=url,
                        content=element.get_text(strip=True)[:1000],
                        source=target['name']
                    )
                    data_list.append(data)
            
            self.logger.info(f"{target['name']}から{len(data_list)}件のデータを収集")
            
        except Exception as e:
            self.logger.error(f"スクレイピングエラー ({target['name']}): {e}")
        
        return data_list
    
    def save_data(self, data_list: List[CollectedData]):
        """データをデータベースに保存"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        duplicate_count = 0
        
        for data in data_list:
            try:
                cursor.execute("""
                    INSERT INTO scraped_data 
                    (title, url, content, source, collected_at, hash_value)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    data.title, data.url, data.content,
                    data.source, data.collected_at, data.hash_value
                ))
                saved_count += 1
            except sqlite3.IntegrityError:
                duplicate_count += 1
        
        conn.commit()
        conn.close()
        self.logger.info(f"保存: {saved_count}件, 重複除外: {duplicate_count}件")
    
    def export_to_csv(self, output_file: str = "output/scraped_data.csv"):
        """データをCSVエクスポート"""
        Path(output_file).parent.mkdir(exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT title, url, source, collected_at 
            FROM scraped_data 
            ORDER BY collected_at DESC 
            LIMIT 100
        """)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Title', 'URL', 'Source', 'Collected At'])
            writer.writerows(cursor.fetchall())
        
        conn.close()
        self.logger.info(f"CSV出力完了: {output_file}")
    
    def run_collection(self):
        """データ収集実行"""
        self.logger.info("データ収集開始")
        
        all_data = []
        for target in self.targets:
            data_list = self.scrape_website(target)
            all_data.extend(data_list)
            time.sleep(2)  # レート制限
        
        if all_data:
            self.save_data(all_data)
            self.export_to_csv()
        
        self.logger.info(f"収集完了: 合計{len(all_data)}件")

if __name__ == "__main__":
    scraper = WebScraper()
    scraper.run_collection()
