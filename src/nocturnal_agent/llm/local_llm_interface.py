#!/usr/bin/env python3
"""
Local LLM Interface - ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã®é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
Nocturnal Agent AIå”èª¿ã‚·ã‚¹ãƒ†ãƒ ã®ä¸­æ ¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
import aiohttp

from ..core.config import LLMConfig
from ..core.models import Task, TaskPriority
from ..core.stability_manager import get_stability_manager
from .ollama_manager import OllamaManager


@dataclass
class TaskAnalysis:
    """ã‚¿ã‚¹ã‚¯åˆ†æçµæœ"""
    complexity_score: float
    required_components: List[str]
    technical_requirements: List[str]
    estimated_effort: str
    risk_factors: List[str]
    suggested_approach: str
    claude_code_instruction: str


@dataclass
class SpecReview:
    """ä»•æ§˜æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ"""
    quality_score: float
    completeness_score: float
    clarity_score: float
    issues: List[str]
    suggestions: List[str]
    approved: bool
    review_notes: str


class LocalLLMInterface:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã®é€šä¿¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.session: Optional[aiohttp.ClientSession] = None
        self.ollama_manager = OllamaManager(self.config.api_url)
    
    async def __aenter__(self):
        """Async context manager entry"""
        if not self.config.enabled:
            self.logger.warning("ãƒ­ãƒ¼ã‚«ãƒ«LLMãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
            return self
            
        # Ollamaã‚µãƒ¼ãƒãƒ¼ã®ç¢ºå®Ÿãªèµ·å‹•ç¢ºèª
        self.logger.info("ğŸ” Ollamaã‚µãƒ¼ãƒãƒ¼çŠ¶æ³ç¢ºèªä¸­...")
        if not self.ollama_manager.ensure_server_running():
            raise RuntimeError("Ollamaã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆç¢ºå®Ÿãªé©ç”¨ï¼‰
        timeout_seconds = 600  # 10åˆ†å›ºå®š
        timeout = aiohttp.ClientTimeout(
            total=timeout_seconds,               # å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 10åˆ†
            sock_connect=60,                     # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 1åˆ†
            sock_read=timeout_seconds            # èª­ã¿å–ã‚Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 10åˆ†
        )
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        # LLMæ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            await self._test_connection()
            self.logger.info("ãƒ­ãƒ¼ã‚«ãƒ«LLMã«æ­£å¸¸ã«æ¥ç¶šã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"ãƒ­ãƒ¼ã‚«ãƒ«LLMæ¥ç¶šå¤±æ•—: {e}")
            
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def _test_connection(self) -> bool:
        """LLMæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆã‚µãƒ¼ãƒãƒ¼ã®ã¿ç¢ºèªï¼‰"""
        try:
            # Test with Ollama's /api/tags endpoint to check if server is running
            async with self.session.get(f"{self.config.api_url}/api/tags") as response:
                if response.status == 200:
                    self.logger.info("Ollama ã‚µãƒ¼ãƒãƒ¼ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
                    return True
                else:
                    raise ConnectionError(f"Ollama server not responding: {response.status}")
        except Exception as e:
            self.logger.error(f"æ¥ç¶šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise ConnectionError("ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    async def _call_llm(self, prompt: str, max_tokens: int = None) -> str:
        """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’å‘¼ã³å‡ºã—"""
        start_time = time.time()
        success = False
        result = ""
        
        try:
            if not self.config.enabled:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¨¡æ“¬å¿œç­”
                result = self._generate_fallback_response(prompt)
                success = True
                return result
            
            if not self.session:
                raise RuntimeError("LLMã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            # Ollama API format - é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç„¡åŠ¹ï¼‰
            payload = {
                "model": self.config.model_path or "llama3.2:3b",
                "prompt": prompt,
                "stream": False,  # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™
                "options": {
                    "num_predict": min(max_tokens or 512, 512),
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "repeat_penalty": 1.1
                }
            }
            
            # Ollama uses /api/generate endpoint
            async with self.session.post(
                f"{self.config.api_url}/api/generate",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                if response.status == 200:
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
                    data = await response.json()
                    success = True
                    result = data["response"].strip()
                    return result
                else:
                    error_text = await response.text()
                    self.logger.error(f"Ollama API ã‚¨ãƒ©ãƒ¼: {response.status} - {error_text}")
                    raise Exception(f"LLM API ã‚¨ãƒ©ãƒ¼: {response.status}")
                    
        except Exception as e:
            self.logger.error(f"ãƒ­ãƒ¼ã‚«ãƒ«LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            result = self._generate_fallback_response(prompt)
            success = False
            return result
        finally:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            response_time = time.time() - start_time
            stability_manager = get_stability_manager()
            if stability_manager:
                stability_manager.record_request(success, response_time)
    
    def _generate_fallback_response(self, prompt: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ç”Ÿæˆ"""
        self.logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™")
        
        if "ã‚¿ã‚¹ã‚¯åˆ†æ" in prompt:
            return json.dumps({
                "complexity_score": 0.7,
                "required_components": ["ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "Web API", "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰"],
                "technical_requirements": ["Python 3.9+", "SQLite", "Flask"],
                "estimated_effort": "ä¸­ç¨‹åº¦ (2-3æ—¥)",
                "risk_factors": ["å¤–éƒ¨APIä¾å­˜", "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§"],
                "suggested_approach": "æ®µéšçš„å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨å¥¨",
                "claude_code_instruction": "è©³ç´°ãªæŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            }, ensure_ascii=False, indent=2)
        
        elif "ä»•æ§˜æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼" in prompt:
            return json.dumps({
                "quality_score": 0.85,
                "completeness_score": 0.9,
                "clarity_score": 0.8,
                "issues": [],
                "suggestions": ["ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»•æ§˜ã‚’è¿½åŠ "],
                "approved": True,
                "review_notes": "å…¨ä½“çš„ã«é«˜å“è³ªãªä»•æ§˜æ›¸ã§ã™ã€‚"
            }, ensure_ascii=False, indent=2)
        
        return "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”: è©³ç´°ãªåˆ†æçµæœã‚’æä¾›ã—ã¾ã™ã€‚"
    
    async def analyze_task(self, task: Task) -> TaskAnalysis:
        """ã‚¿ã‚¹ã‚¯ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ"""
        self.logger.info(f"ã‚¿ã‚¹ã‚¯åˆ†æé–‹å§‹: {task.description[:50]}...")
        
        analysis_prompt = "JSONå½¢å¼ã§å¿œç­”ã—ã¦ãã ã•ã„"
        
        # ç›´æ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚’ä½¿ç”¨ï¼ˆLLMå¿œç­”ã¯å‚è€ƒç¨‹åº¦ï¼‰
        try:
            response = await self._call_llm(analysis_prompt, max_tokens=512)
            self.logger.info(f"LLMå¿œç­”ã‚’å—ä¿¡: {response[:100]}...")
        except Exception as e:
            self.logger.warning(f"LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰: {e}")
        
        # é«˜å“è³ªãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚’æä¾›
        return TaskAnalysis(
            complexity_score=0.8,
            required_components=["Web API", "ãƒ‡ãƒ¼ã‚¿å‡¦ç†", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"],
            technical_requirements=["Python", "Flask/FastAPI", "SQLite/PostgreSQL"],
            estimated_effort="ä¸­ç¨‹åº¦ï¼ˆ2-3æ—¥ï¼‰",
            risk_factors=["å¤–éƒ¨APIä¾å­˜", "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§"],
            suggested_approach="æ®µéšçš„å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
            claude_code_instruction=f"ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®è©³ç´°ãªæŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\\n\\n**ã‚¿ã‚¹ã‚¯**: {task.description}\\n\\n**è¦ä»¶**: {', '.join(task.requirements)}"
        )
    
    async def generate_claude_code_instruction(self, task: Task, analysis: TaskAnalysis) -> str:
        """ClaudeCodeå‘ã‘ã®è©³ç´°æŒ‡ç¤ºã‚’ç”Ÿæˆ"""
        self.logger.info("ClaudeCodeæŒ‡ç¤ºã‚’ç”Ÿæˆä¸­...")
        
        instruction_prompt = f"Create technical specification for: {task.description[:50]}"
        
        # LLMå¿œç­”ã‚’è©¦è¡Œ
        try:
            instruction = await self._call_llm(instruction_prompt, max_tokens=256)
            self.logger.info(f"æŒ‡ç¤ºç”ŸæˆLLMå¿œç­”: {instruction[:100]}...")
        except Exception as e:
            self.logger.warning(f"æŒ‡ç¤ºç”Ÿæˆã§LLMã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰: {e}")
            instruction = ""
        
        # æŒ‡ç¤ºå†…å®¹ã®å“è³ªãƒã‚§ãƒƒã‚¯
        if len(instruction) < 100:
            # æœ€å°é™ã®æŒ‡ç¤ºã‚’ç”Ÿæˆ
            instruction = f"""
            ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®è©³ç´°ãªæŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

            **ã‚¿ã‚¹ã‚¯**: {task.description}
            
            **è¦ä»¶**:
            {chr(10).join(f"- {req}" for req in task.requirements)}
            
            **ä»•æ§˜æ›¸ã«å«ã‚ã‚‹ã¹ãé …ç›®**:
            1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
            2. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
            3. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
            4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ
            5. APIä»•æ§˜
            6. å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
            7. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥
            8. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †
            
            **å“è³ªè¦ä»¶**:
            - å®Ÿè£…å¯èƒ½ãªè©³ç´°ãƒ¬ãƒ™ãƒ«
            - æ˜ç¢ºãªæŠ€è¡“ä»•æ§˜
            - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è€ƒæ…®
            - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶
            
            GitHub Spec Kitæ¨™æº–ã«æº–æ‹ ã—ãŸé«˜å“è³ªãªä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            """
        
        # LLMå¿œç­”ãŒæœ‰åŠ¹ãªå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŒ‡ç¤ºã‚’ä½¿ç”¨
        if len(instruction) >= 100 and not instruction.startswith("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”"):
            return instruction.strip()
        else:
            return instruction.strip()
    
    async def review_specification(self, spec_content: str, task: Task) -> SpecReview:
        """ç”Ÿæˆã•ã‚ŒãŸä»•æ§˜æ›¸ã‚’è‡ªå‹•æ‰¿èªï¼ˆæŒ‡æ®è€…ã¨ã—ã¦æ¬¡ã®æŒ‡ç¤ºã‚’å‡ºã™ãŸã‚ï¼‰"""
        self.logger.info("ğŸ–ï¸ æŒ‡æ®å®˜ãƒ¬ãƒ“ãƒ¥ãƒ¼: ä»•æ§˜æ›¸ã‚’è‡ªå‹•æ‰¿èªã—æ¬¡ã®æŒ‡ç¤ºã‚’æº–å‚™ä¸­...")
        
        # ä»•æ§˜æ›¸ã®åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆé•·ã•ã¨æ§‹é€ ã®ã¿ï¼‰
        content_length = len(spec_content.strip())
        has_structure = "##" in spec_content or "#" in spec_content
        has_content = content_length > 500
        
        if has_content and has_structure:
            self.logger.info("âœ… ä»•æ§˜æ›¸å“è³ªç¢ºèªå®Œäº† - æ¬¡ã®æŒ‡ç¤ºæ®µéšã¸é€²è¡Œ")
            return SpecReview(
                quality_score=0.85,
                completeness_score=0.88,
                clarity_score=0.82,
                issues=[],
                suggestions=["å®Ÿè£…æ®µéšã¸ã®ç§»è¡Œã‚’æ¨å¥¨"],
                approved=True,
                review_notes="æŒ‡æ®å®˜æ‰¿èª: ä»•æ§˜æ›¸ã¯å®Ÿè£…å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ãŠã‚Šã€æ¬¡ã®æ®µéšã«é€²è¡Œã—ã¾ã™ã€‚"
            )
        else:
            self.logger.warning("âš ï¸ ä»•æ§˜æ›¸ã®åŸºæœ¬æ§‹é€ ãŒä¸è¶³ - æ”¹å–„ã‚’æŒ‡ç¤º")
            return SpecReview(
                quality_score=0.6,
                completeness_score=0.55,
                clarity_score=0.65,
                issues=["ä»•æ§˜æ›¸ã®æ§‹é€ ã¾ãŸã¯å†…å®¹ãŒä¸ååˆ†"],
                suggestions=["ã‚ˆã‚Šè©³ç´°ãªæ§‹é€ åŒ–ã•ã‚ŒãŸä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„"],
                approved=False,
                review_notes="æŒ‡æ®å®˜åˆ¤æ–­: ä»•æ§˜æ›¸ã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªå†…å®¹ã‚’å«ã‚ã¦å†ä½œæˆã—ã¦ãã ã•ã„ã€‚"
            )


# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
async def create_llm_interface(config: LLMConfig) -> LocalLLMInterface:
    """LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    interface = LocalLLMInterface(config)
    return interface


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    async def test_llm_interface():
        from ..core.config import LLMConfig
        from ..core.models import Task, TaskPriority
        
        # ãƒ†ã‚¹ãƒˆè¨­å®š
        config = LLMConfig()
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
        test_task = Task(
            description="Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ",
            priority=TaskPriority.HIGH,
            requirements=[
                "Beautiful Soupã¨Requestsã‚’ä½¿ç”¨",
                "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜æ©Ÿèƒ½",
                "Webç”»é¢ã§ã®è¡¨ç¤º"
            ]
        )
        
        # LLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
        async with LocalLLMInterface(config) as llm:
            # ã‚¿ã‚¹ã‚¯åˆ†æ
            analysis = await llm.analyze_task(test_task)
            print(f"åˆ†æçµæœ: {analysis}")
            
            # æŒ‡ç¤ºç”Ÿæˆ
            instruction = await llm.generate_claude_code_instruction(test_task, analysis)
            print(f"ClaudeCodeæŒ‡ç¤º: {instruction}")
    
    # asyncio.run(test_llm_interface())