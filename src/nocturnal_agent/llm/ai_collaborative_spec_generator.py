#!/usr/bin/env python3
"""
AI Collaborative Specification Generator
ãƒ­ãƒ¼ã‚«ãƒ«LLM â†” ClaudeCode å”èª¿ã«ã‚ˆã‚‹é«˜å“è³ªä»•æ§˜æ›¸ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
from pathlib import Path

from .local_llm_interface import LocalLLMInterface, TaskAnalysis, SpecReview
from .claude_code_interface import ClaudeCodeInterface, SpecificationDocument, load_claude_config
from ..core.config import LLMConfig
from ..core.models import Task
from ..design.spec_kit_integration import TechnicalSpec, SpecStatus


@dataclass
class CollaborationReport:
    """AIå”èª¿ãƒ¬ãƒãƒ¼ãƒˆ"""
    task_id: str
    collaboration_id: str
    start_time: datetime
    end_time: Optional[datetime]
    total_duration: float
    
    # ãƒ•ã‚§ãƒ¼ã‚ºæƒ…å ±
    analysis_phase: Dict[str, Any]
    instruction_phase: Dict[str, Any] 
    generation_phase: Dict[str, Any]
    review_phase: Dict[str, Any]
    enhancement_phase: Optional[Dict[str, Any]]
    
    # æœ€çµ‚çµæœ
    final_specification: Optional[SpecificationDocument]
    quality_metrics: Dict[str, float]
    collaboration_success: bool
    issues_encountered: List[str]
    
    # AIä½¿ç”¨çŠ¶æ³
    local_llm_calls: int
    claude_code_calls: int
    total_tokens_estimated: int


class AICollaborativeSpecGenerator:
    """AIå”èª¿ä»•æ§˜æ›¸ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, llm_config: LLMConfig):
        self.llm_config = llm_config
        self.claude_config = load_claude_config()
        self.logger = logging.getLogger(__name__)
        
        # å”èª¿è¨­å®š
        self.max_enhancement_iterations = 2
        self.quality_threshold = 0.85
        self.min_spec_length = 1000  # æœ€å°æ–‡å­—æ•°
        
        # çµ±è¨ˆæƒ…å ±
        self.total_collaborations = 0
        self.successful_collaborations = 0
    
    async def generate_specification_collaboratively(
        self, 
        task: Task,
        collaboration_options: Dict = None
    ) -> Tuple[TechnicalSpec, CollaborationReport]:
        """AIå”èª¿ã«ã‚ˆã‚‹ä»•æ§˜æ›¸ç”Ÿæˆãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼ï¼ˆæŒ‡æ®å®˜å‹ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰"""
        
        collaboration_id = f"collab_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        start_time = datetime.now()
        
        self.logger.info(f"ğŸ¯ AIå”èª¿ä»•æ§˜æ›¸ç”Ÿæˆé–‹å§‹ [ID: {collaboration_id}]")
        self.logger.info(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯: {task.description[:100]}...")
        self.logger.info(f"ğŸ–ï¸ ãƒ­ãƒ¼ã‚«ãƒ«LLM: æŒ‡æ®å®˜å½¹ | ğŸ“ ClaudeCode: æŠ€è¡“å®Ÿè¡Œå½¹")
        
        # ãƒ¬ãƒãƒ¼ãƒˆåˆæœŸåŒ–
        report = CollaborationReport(
            task_id=task.id,
            collaboration_id=collaboration_id,
            start_time=start_time,
            end_time=None,
            total_duration=0.0,
            analysis_phase={},
            instruction_phase={},
            generation_phase={},
            review_phase={},
            enhancement_phase=None,
            final_specification=None,
            quality_metrics={},
            collaboration_success=False,
            issues_encountered=[],
            local_llm_calls=0,
            claude_code_calls=0,
            total_tokens_estimated=0
        )
        
        try:
            # ãƒ•ã‚§ãƒ¼ã‚º1: ã‚¿ã‚¹ã‚¯åˆ†æ
            analysis_result, analysis_report = await self._phase_1_task_analysis(task)
            report.analysis_phase = analysis_report
            report.local_llm_calls += 1
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: æŒ‡ç¤ºç”Ÿæˆ
            instructions, instruction_report = await self._phase_2_instruction_generation(task, analysis_result)
            report.instruction_phase = instruction_report
            report.claude_code_calls += 1
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: ä»•æ§˜æ›¸ç”Ÿæˆ
            specification, generation_report = await self._phase_3_specification_generation(task, instructions)
            report.generation_phase = generation_report
            report.local_llm_calls += 1
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: ä»•æ§˜æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå¼•æ•°ã‚’æ­£ã—ãæ¸¡ã™ï¼‰
            review_result, review_report = await self._phase_4_specification_review(task, specification)
            report.review_phase = review_report
            report.claude_code_calls += 1
            
            # ãƒ•ã‚§ãƒ¼ã‚º5: åå¾©çš„æ”¹å–„
            final_spec, enhancement_report = await self._phase_5_iterative_enhancement(task, specification, review_result)
            report.enhancement_phase = enhancement_report
            report.local_llm_calls += 1
            
            # æœ€çµ‚ä»•æ§˜æ›¸ã‚’TechnicalSpecã«å¤‰æ›
            final_tech_spec = self._convert_to_technical_spec(final_spec, task, analysis_result)
            
            # ãƒ¬ãƒãƒ¼ãƒˆå®Œäº†
            report.final_specification = final_spec
            report.quality_metrics = self._calculate_final_quality_metrics(final_spec, review_result)
            report.collaboration_success = True
            
            self.successful_collaborations += 1
            
        except Exception as e:
            self.logger.error(f"AIå”èª¿ã‚¨ãƒ©ãƒ¼: {e}")
            report.issues_encountered.append(str(e))
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»•æ§˜æ›¸ç”Ÿæˆ
            final_tech_spec = self._generate_fallback_technical_spec(task)
            
        finally:
            report.end_time = datetime.now()
            report.total_duration = (report.end_time - start_time).total_seconds()
            report.total_tokens_estimated = self._estimate_total_tokens(report)
            
            self.total_collaborations += 1
            
            self.logger.info(f"âœ… AIå”èª¿å®Œäº† [{collaboration_id}] - {report.total_duration:.2f}ç§’")
            self.logger.info(f"ğŸ“Š ãƒ­ãƒ¼ã‚«ãƒ«LLMå‘¼ã³å‡ºã—: {report.local_llm_calls}å› | ClaudeCodeå‘¼ã³å‡ºã—: {report.claude_code_calls}å›")
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            await self._save_collaboration_report(report)
        
        return final_tech_spec, report
    
    async def _phase_1_task_analysis(self, task: Task) -> Tuple[TaskAnalysis, Dict]:
        """ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†æ"""
        phase_start = datetime.now()
        self.logger.info("ğŸ” Phase 1: ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯åˆ†æ")
        
        try:
            async with LocalLLMInterface(self.llm_config) as llm:
                analysis = await llm.analyze_task(task)
                
                phase_report = {
                    "phase": "task_analysis",
                    "duration": (datetime.now() - phase_start).total_seconds(),
                    "success": True,
                    "complexity_score": analysis.complexity_score,
                    "components_identified": len(analysis.required_components),
                    "risks_identified": len(analysis.risk_factors)
                }
                
                self.logger.info(f"ğŸ“Š åˆ†æå®Œäº†: è¤‡é›‘æ€§ã‚¹ã‚³ã‚¢ {analysis.complexity_score:.2f}")
                return analysis, phase_report
                
        except Exception as e:
            self.logger.error(f"Phase 1 ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ
            fallback_analysis = TaskAnalysis(
                complexity_score=0.7,
                required_components=["ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ "],
                technical_requirements=["Python"],
                estimated_effort="ä¸­ç¨‹åº¦",
                risk_factors=["æŠ€è¡“çš„è¤‡é›‘æ€§"],
                suggested_approach="æ®µéšçš„å®Ÿè£…",
                claude_code_instruction=f"ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®æŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„: {task.description}"
            )
            
            phase_report = {
                "phase": "task_analysis",
                "duration": (datetime.now() - phase_start).total_seconds(),
                "success": False,
                "error": str(e),
                "fallback_used": True
            }
            
            return fallback_analysis, phase_report
    
    async def _phase_2_instruction_generation(self, task: Task, analysis: TaskAnalysis) -> Tuple[str, Dict]:
        """ãƒ•ã‚§ãƒ¼ã‚º2: ClaudeCodeæŒ‡ç¤ºç”Ÿæˆ"""
        phase_start = datetime.now()
        self.logger.info("ğŸ“ Phase 2: ClaudeCodeæŒ‡ç¤ºç”Ÿæˆ")
        
        try:
            async with LocalLLMInterface(self.llm_config) as llm:
                instruction = await llm.generate_claude_code_instruction(task, analysis)
                
                phase_report = {
                    "phase": "instruction_generation",
                    "duration": (datetime.now() - phase_start).total_seconds(),
                    "success": True,
                    "instruction_length": len(instruction),
                    "based_on_analysis": True
                }
                
                self.logger.info(f"ğŸ“‹ æŒ‡ç¤ºç”Ÿæˆå®Œäº†: {len(instruction)}æ–‡å­—")
                return instruction, phase_report
                
        except Exception as e:
            self.logger.error(f"Phase 2 ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŒ‡ç¤º
            fallback_instruction = f"""
            ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®è©³ç´°ãªæŠ€è¡“ä»•æ§˜æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

            **ã‚¿ã‚¹ã‚¯**: {task.description}
            **è¦ä»¶**: {', '.join(task.requirements)}
            
            é«˜å“è³ªã§å®Ÿè£…å¯èƒ½ãªæŠ€è¡“ä»•æ§˜æ›¸ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
            """
            
            phase_report = {
                "phase": "instruction_generation", 
                "duration": (datetime.now() - phase_start).total_seconds(),
                "success": False,
                "error": str(e),
                "fallback_used": True
            }
            
            return fallback_instruction.strip(), phase_report
    
    async def _phase_3_specification_generation(self, task: Task, instruction: str) -> Tuple[SpecificationDocument, Dict]:
        """ãƒ•ã‚§ãƒ¼ã‚º3: ClaudeCodeã«ã‚ˆã‚‹ä»•æ§˜æ›¸ç”Ÿæˆ"""
        phase_start = datetime.now()
        self.logger.info("ğŸ¯ Phase 3: ClaudeCodeã«ã‚ˆã‚‹ä»•æ§˜æ›¸ç”Ÿæˆ")
        
        try:
            async with ClaudeCodeInterface(self.claude_config) as claude:
                specification = await claude.generate_specification_document(instruction, task)
                
                phase_report = {
                    "phase": "specification_generation",
                    "duration": (datetime.now() - phase_start).total_seconds(),
                    "success": True,
                    "spec_word_count": specification.word_count,
                    "generation_time": specification.generation_time,
                    "sections_generated": len(specification.sections)
                }
                
                self.logger.info(f"ğŸ“„ ä»•æ§˜æ›¸ç”Ÿæˆå®Œäº†: {specification.word_count}èª, {len(specification.sections)}ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
                return specification, phase_report
                
        except Exception as e:
            self.logger.error(f"Phase 3 ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»•æ§˜æ›¸
            fallback_content = f"""
            # {task.description} - æŠ€è¡“ä»•æ§˜æ›¸
            
            ## æ¦‚è¦
            æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™å®Ÿè£…ã‚’æä¾›ã—ã¾ã™ï¼š
            {chr(10).join(f'- {req}' for req in task.requirements)}
            
            ## å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
            - æ®µéšçš„ãªå®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            - å“è³ªé‡è¦–ã®é–‹ç™º
            - é©åˆ‡ãªãƒ†ã‚¹ãƒˆå®Ÿè£…
            """
            
            fallback_spec = SpecificationDocument(
                title=f"{task.description} (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)",
                content=fallback_content,
                sections={"æ¦‚è¦": "åŸºæœ¬çš„ãªä»•æ§˜æ¦‚è¦"},
                metadata={"fallback": True},
                quality_indicators={"completeness_score": 0.5},
                generation_time=0.1,
                word_count=len(fallback_content.split())
            )
            
            phase_report = {
                "phase": "specification_generation",
                "duration": (datetime.now() - phase_start).total_seconds(),
                "success": False,
                "error": str(e),
                "fallback_used": True
            }
            
            return fallback_spec, phase_report
    
    async def _phase_4_specification_review(self, task: Task, specification: SpecificationDocument) -> Tuple[SpecReview, Dict]:
        """ãƒ•ã‚§ãƒ¼ã‚º4: ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã‚ˆã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼"""
        phase_start = datetime.now()
        self.logger.info("ğŸ” Phase 4: ãƒ­ãƒ¼ã‚«ãƒ«LLMã«ã‚ˆã‚‹ä»•æ§˜æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        try:
            async with LocalLLMInterface(self.llm_config) as llm:
                review = await llm.review_specification(specification.content, task)
                
                phase_report = {
                    "phase": "specification_review",
                    "duration": (datetime.now() - phase_start).total_seconds(),
                    "success": True,
                    "quality_score": review.quality_score,
                    "approved": review.approved,
                    "issues_found": len(review.issues),
                    "suggestions_made": len(review.suggestions)
                }
                
                approval_status = "æ‰¿èª" if review.approved else "è¦æ”¹å–„"
                self.logger.info(f"ğŸ“‹ ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†: {approval_status} (å“è³ª: {review.quality_score:.2f})")
                return review, phase_report
                
        except Exception as e:
            self.logger.error(f"Phase 4 ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼
            fallback_review = SpecReview(
                quality_score=0.75,
                completeness_score=0.8,
                clarity_score=0.7,
                issues=[],
                suggestions=["è©³ç´°ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã®è¿½åŠ "],
                approved=True,
                review_notes="åŸºæœ¬çš„ãªå“è³ªè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™"
            )
            
            phase_report = {
                "phase": "specification_review",
                "duration": (datetime.now() - phase_start).total_seconds(),
                "success": False,
                "error": str(e),
                "fallback_used": True
            }
            
            return fallback_review, phase_report
    
    async def _phase_5_iterative_enhancement(
        self, 
        task: Task, 
        specification: SpecificationDocument, 
        review: SpecReview
    ) -> Tuple[SpecificationDocument, Dict]:
        """ãƒ•ã‚§ãƒ¼ã‚º5: åå¾©çš„æ”¹å–„"""
        phase_start = datetime.now()
        self.logger.info("ğŸ”„ Phase 5: ä»•æ§˜æ›¸ã®åå¾©çš„æ”¹å–„")
        
        try:
            async with ClaudeCodeInterface(self.claude_config) as claude:
                enhanced_spec = await claude.enhance_specification_iteratively(
                    specification, 
                    review.suggestions
                )
                
                phase_report = {
                    "phase": "iterative_enhancement",
                    "duration": (datetime.now() - phase_start).total_seconds(),
                    "success": True,
                    "enhancements_applied": len(review.suggestions),
                    "word_count_before": specification.word_count,
                    "word_count_after": enhanced_spec.word_count
                }
                
                self.logger.info(f"âœ¨ æ”¹å–„å®Œäº†: {enhanced_spec.word_count}èª (+{enhanced_spec.word_count - specification.word_count})")
                return enhanced_spec, phase_report
                
        except Exception as e:
            self.logger.error(f"Phase 5 ã‚¨ãƒ©ãƒ¼: {e}")
            
            phase_report = {
                "phase": "iterative_enhancement",
                "duration": (datetime.now() - phase_start).total_seconds(),
                "success": False,
                "error": str(e),
                "fallback_used": True
            }
            
            # å…ƒã®ä»•æ§˜æ›¸ã‚’è¿”ã™
            return specification, phase_report
    
    def _convert_to_technical_spec(
        self, 
        specification: SpecificationDocument, 
        task: Task, 
        analysis: TaskAnalysis
    ) -> TechnicalSpec:
        """SpecificationDocumentã‚’TechnicalSpecã«å¤‰æ›"""
        from ..design.spec_kit_integration import (
            SpecMetadata, SpecRequirement, SpecDesign, SpecImplementation,
            SpecType
        )
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = SpecMetadata(
            title=specification.title,
            status=SpecStatus.APPROVED,
            spec_type=SpecType.FEATURE,
            authors=["AI Collaboration System", "LocalLLM", "ClaudeCode"],
            tags=["ai-generated", "collaborative", task.priority.value]
        )
        
        # è¦ä»¶å¤‰æ›
        requirements = []
        for i, req_text in enumerate(task.requirements):
            requirement = SpecRequirement(
                id=f"AI-REQ-{i+1:03d}",
                title=f"AIåˆ†æè¦ä»¶ {i+1}",
                description=req_text,
                priority="high" if task.priority.value == "high" else "medium"
            )
            requirements.append(requirement)
        
        # è¨­è¨ˆæƒ…å ±
        design = SpecDesign(
            overview=specification.content,
            architecture={
                "ai_collaboration": True,
                "complexity_score": analysis.complexity_score,
                "required_components": analysis.required_components,
                "quality_target": 0.9
            },
            components=analysis.required_components,
            interfaces=[]
        )
        
        # å®Ÿè£…æƒ…å ±
        implementation = SpecImplementation(
            approach=analysis.suggested_approach,
            timeline={"total_effort": analysis.estimated_effort},
            testing_strategy="AIå”èª¿å“è³ªä¿è¨¼",
            risks=[{"risk": risk, "mitigation": "é©åˆ‡ãªå¯¾ç­–ã‚’å®Ÿè£…"} for risk in analysis.risk_factors]
        )
        
        return TechnicalSpec(
            metadata=metadata,
            summary=task.description,
            motivation=f"ã‚¿ã‚¹ã‚¯è¦æ±‚: {task.description}\nå“è³ªç›®æ¨™: {task.estimated_quality}\nè¤‡é›‘æ€§: {analysis.complexity_score}",
            requirements=requirements,
            design=design,
            implementation=implementation
        )
    
    def _generate_fallback_technical_spec(self, task: Task) -> TechnicalSpec:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ€è¡“ä»•æ§˜ç”Ÿæˆ"""
        from ..design.spec_kit_integration import (
            SpecMetadata, SpecRequirement, SpecDesign, SpecImplementation,
            SpecType
        )
        
        metadata = SpecMetadata(
            title=f"{task.description} (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»•æ§˜)",
            status=SpecStatus.DRAFT,
            spec_type=SpecType.FEATURE,
            authors=["Fallback Generator"],
            tags=["fallback", "basic"]
        )
        
        requirements = [
            SpecRequirement(
                id="FALLBACK-001",
                title="åŸºæœ¬å®Ÿè£…",
                description=task.description,
                priority="medium"
            )
        ]
        
        design = SpecDesign(
            overview="åŸºæœ¬çš„ãªå®Ÿè£…ä»•æ§˜",
            architecture={"approach": "simple"},
            components=[],
            interfaces=[]
        )
        
        implementation = SpecImplementation(
            approach="æ¨™æº–å®Ÿè£…",
            timeline={"estimate": "unknown"},
            testing_strategy="åŸºæœ¬ãƒ†ã‚¹ãƒˆ"
        )
        
        return TechnicalSpec(
            metadata=metadata,
            summary=task.description,
            motivation=f"è¦æ±‚: {task.description}",
            requirements=requirements,
            design=design,
            implementation=implementation
        )
    
    def _calculate_final_quality_metrics(
        self, 
        specification: SpecificationDocument, 
        review: SpecReview
    ) -> Dict[str, float]:
        """æœ€çµ‚å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        return {
            "overall_quality": (review.quality_score + review.completeness_score + review.clarity_score) / 3,
            "specification_quality": sum(specification.quality_indicators.values()) / len(specification.quality_indicators),
            "collaboration_efficiency": 1.0 if review.approved else 0.7,
            "word_density": min(specification.word_count / 1000, 1.0)
        }
    
    def _estimate_total_tokens(self, report: CollaborationReport) -> int:
        """ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®š"""
        # ç°¡æ˜“æ¨å®š: 1èª â‰ˆ 1.3ãƒˆãƒ¼ã‚¯ãƒ³
        base_tokens = report.local_llm_calls * 500  # ãƒ­ãƒ¼ã‚«ãƒ«LLMå‘¼ã³å‡ºã—ã‚ãŸã‚Š
        claude_tokens = report.claude_code_calls * 2000  # ClaudeCodeå‘¼ã³å‡ºã—ã‚ãŸã‚Š
        
        if report.final_specification:
            claude_tokens += report.final_specification.word_count * 1.3
        
        return int(base_tokens + claude_tokens)
    
    async def _save_collaboration_report(self, report: CollaborationReport) -> None:
        """å”èª¿ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        reports_dir = Path("logs/ai_collaboration")
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = reports_dir / f"collaboration_{report.collaboration_id}.json"
        
        # dataclassã‚’JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        report_dict = asdict(report)
        report_dict["start_time"] = report.start_time.isoformat()
        if report.end_time:
            report_dict["end_time"] = report.end_time.isoformat()
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"ğŸ“‹ å”èª¿ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        except Exception as e:
            self.logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
async def generate_collaborative_specification(task: Task) -> Tuple[TechnicalSpec, CollaborationReport]:
    """AIå”èª¿ä»•æ§˜æ›¸ç”Ÿæˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    from ..core.config import LLMConfig
    
    llm_config = LLMConfig()
    generator = AICollaborativeSpecGenerator(llm_config)
    
    return await generator.generate_specification_collaboratively(task)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    async def test_collaborative_generation():
        from ..core.models import Task, TaskPriority
        
        test_task = Task(
            description="AIå”èª¿ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ ",
            priority=TaskPriority.HIGH,
            requirements=[
                "ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ClaudeCodeã®å”èª¿",
                "é«˜å“è³ªãªä»•æ§˜æ›¸ç”Ÿæˆ",
                "åå¾©çš„æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹"
            ]
        )
        
        spec, report = await generate_collaborative_specification(test_task)
        
        print(f"ä»•æ§˜æ›¸ã‚¿ã‚¤ãƒˆãƒ«: {spec.metadata.title}")
        print(f"å”èª¿æˆåŠŸ: {report.collaboration_success}")
        print(f"å®Ÿè¡Œæ™‚é–“: {report.total_duration:.2f}ç§’")
        print(f"å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹: {report.quality_metrics}")
    
    # asyncio.run(test_collaborative_generation())