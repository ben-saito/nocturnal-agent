#!/usr/bin/env python3
"""
ç°¡æ˜“ç‰ˆ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import json
import os
import sys
from pathlib import Path

async def execute_integrated_ai_news_system():
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ"""
    
    print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œã‚’é–‹å§‹...")
    
    # ai-news-digãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹
    target_project = "/Users/tsutomusaito/git/ai-news-dig"
    os.chdir(target_project)
    
    print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    print("ğŸ“ 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ...")
    
    database_code = '''import sqlite3
import hashlib
from datetime import datetime
from typing import List, Dict, Optional

class AINewsDatabase:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "ai_news.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS news_articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            content TEXT,
            url TEXT UNIQUE,
            source TEXT,
            published_date TEXT,
            author TEXT,
            keywords TEXT,
            content_hash TEXT UNIQUE,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # çµ„ã¿åˆã‚ã›ææ¡ˆãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS combination_proposals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            news1_id INTEGER,
            news2_id INTEGER,
            common_keywords TEXT,
            proposal_text TEXT,
            potential_applications TEXT,
            confidence_score REAL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (news1_id) REFERENCES news_articles (id),
            FOREIGN KEY (news2_id) REFERENCES news_articles (id)
        )
        """)
        
        conn.commit()
        conn.close()
        print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def add_news_article(self, article: Dict) -> Optional[int]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        content_hash = hashlib.md5(
            (article.get('title', '') + article.get('content', '')).encode()
        ).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
            INSERT INTO news_articles 
            (title, content, url, source, published_date, author, keywords, content_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article.get('title', ''),
                article.get('content', ''),
                article.get('url', ''),
                article.get('source', ''),
                article.get('published_date', ''),
                article.get('author', ''),
                article.get('keywords', ''),
                content_hash
            ))
            
            article_id = cursor.lastrowid
            conn.commit()
            return article_id
            
        except sqlite3.IntegrityError:
            # é‡è¤‡è¨˜äº‹ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            return None
        finally:
            conn.close()
    
    def get_recent_articles(self, limit: int = 50) -> List[Dict]:
        """æœ€è¿‘ã®è¨˜äº‹ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT id, title, content, url, source, published_date, author, keywords
        FROM news_articles 
        ORDER BY created_at DESC 
        LIMIT ?
        """, (limit,))
        
        columns = [description[0] for description in cursor.description]
        articles = []
        
        for row in cursor.fetchall():
            article = dict(zip(columns, row))
            articles.append(article)
        
        conn.close()
        return articles
    
    def save_combination_proposal(self, proposal: Dict) -> int:
        """çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ä¿å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
        INSERT INTO combination_proposals 
        (news1_id, news2_id, common_keywords, proposal_text, potential_applications, confidence_score)
        VALUES (?, ?, ?, ?, ?, ?)
        """, (
            proposal.get('news1_id'),
            proposal.get('news2_id'),
            ','.join(proposal.get('common_keywords', [])),
            proposal.get('proposal_text', ''),
            ','.join(proposal.get('potential_applications', [])),
            proposal.get('confidence_score', 0.5)
        ))
        
        proposal_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return proposal_id
'''
    
    with open("ai_news_database.py", "w", encoding="utf-8") as f:
        f.write(database_code)
    
    # 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
    print("ğŸ“ 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’ä½œæˆ...")
    
    fetcher_code = '''import asyncio
from datetime import datetime
from typing import List, Dict

class AINewsFetcher:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’Webã‹ã‚‰åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ai_keywords = [
            'artificial intelligence', 'AI', 'machine learning', 'ML', 'deep learning',
            'neural network', 'GPT', 'ChatGPT', 'LLM', 'natural language processing',
            'computer vision', 'robotics', 'automation', 'algorithm'
        ]
    
    async def fetch_all_news(self) -> List[Dict]:
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        print("ğŸ“¡ ã‚µãƒ³ãƒ—ãƒ«AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç”Ÿæˆä¸­...")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯RSS/API/ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
        sample_news = [
            {
                'title': 'OpenAI GPT-4ãŒåŒ»ç™‚è¨ºæ–­ã§äººé–“ã®åŒ»å¸«ã‚’ä¸Šå›ã‚‹ç²¾åº¦ã‚’é”æˆ',
                'content': 'æœ€æ–°ã®ç ”ç©¶ã«ã‚ˆã‚Šã€GPT-4ãŒè¤‡é›‘ãªåŒ»ç™‚è¨ºæ–­ã«ãŠã„ã¦å¾“æ¥ã®è¨ºæ–­æ–¹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸã€‚',
                'url': 'https://example.com/gpt4-medical-diagnosis',
                'source': 'AI Medical News',
                'published_date': datetime.now().isoformat(),
                'author': 'Dr. AI Researcher',
                'fetch_method': 'Sample'
            },
            {
                'title': 'Google DeepMindãŒæ–°ã—ã„é‡å­æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç™ºè¡¨',
                'content': 'é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨æ©Ÿæ¢°å­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ãŸé©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å‡¦ç†é€Ÿåº¦ãŒ1000å€å‘ä¸Šã€‚',
                'url': 'https://example.com/deepmind-quantum-ml',
                'source': 'Quantum AI Today',
                'published_date': datetime.now().isoformat(),
                'author': 'Quantum Team',
                'fetch_method': 'Sample'
            },
            {
                'title': 'ãƒ†ã‚¹ãƒ©ã®è‡ªå‹•é‹è»¢AIã€å®Œå…¨è‡ªå‹•é‹è»¢ãƒ¬ãƒ™ãƒ«5ã‚’é”æˆ',
                'content': 'ã‚¤ãƒ¼ãƒ­ãƒ³ãƒ»ãƒã‚¹ã‚¯ãŒç™ºè¡¨ã—ãŸãƒ†ã‚¹ãƒ©ã®æœ€æ–°AIæŠ€è¡“ã«ã‚ˆã‚Šã€å®Œå…¨è‡ªå‹•é‹è»¢ãŒç¾å®Ÿã®ã‚‚ã®ã¨ãªã£ãŸã€‚',
                'url': 'https://example.com/tesla-level5-autonomous',
                'source': 'AutoAI Weekly',
                'published_date': datetime.now().isoformat(),
                'author': 'Tesla AI Team',
                'fetch_method': 'Sample'
            },
            {
                'title': 'Microsoft Copilotã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ç”Ÿç”£æ€§ã‚’300%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒåˆ¤æ˜',
                'content': 'æœ€æ–°ã®èª¿æŸ»ã«ã‚ˆã‚Šã€GitHub Copilotã‚’ä½¿ç”¨ã—ãŸé–‹ç™ºè€…ã®ç”Ÿç”£æ€§ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚',
                'url': 'https://example.com/copilot-productivity-boost',
                'source': 'Developer AI News',
                'published_date': datetime.now().isoformat(),
                'author': 'Microsoft Research',
                'fetch_method': 'Sample'
            },
            {
                'title': 'Amazon AlexaãŒAIæ„Ÿæƒ…èªè­˜æ©Ÿèƒ½ã‚’æ­è¼‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°—æŒã¡ã‚’ç†è§£',
                'content': 'æ–°ã—ã„Alexa AIã¯éŸ³å£°ã‹ã‚‰æ„Ÿæƒ…ã‚’èª­ã¿å–ã‚Šã€ã‚ˆã‚Šäººé–“ã‚‰ã—ã„å¯¾è©±ãŒå¯èƒ½ã«ãªã£ãŸã€‚',
                'url': 'https://example.com/alexa-emotion-ai',
                'source': 'Voice AI Report',
                'published_date': datetime.now().isoformat(),
                'author': 'Amazon AI Lab',
                'fetch_method': 'Sample'
            }
        ]
        
        print(f"âœ… {len(sample_news)} ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return sample_news
    
    def is_ai_related(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.ai_keywords)
'''
    
    with open("ai_news_fetcher.py", "w", encoding="utf-8") as f:
        f.write(fetcher_code)
    
    # 3. çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
    print("ğŸ“ 3. çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ...")
    
    integration_code = '''import asyncio
import sys
import os
from datetime import datetime

# æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ai_news_combination_proposer import AINewsCombinationProposer
from ai_news_database import AINewsDatabase
from ai_news_fetcher import AINewsFetcher

class AINewsIntegratedSystem:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ•ã‚§ãƒƒãƒã€DBç™»éŒ²ã€çµ„ã¿åˆã‚ã›ææ¡ˆã‚’çµ±åˆ"""
    
    def __init__(self):
        self.database = AINewsDatabase()
        self.fetcher = AINewsFetcher()
        self.proposer = AINewsCombinationProposer()
    
    async def run_full_pipeline(self):
        """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹...")
        
        # 1. Webã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
        print("\\nğŸ“¡ Step 1: Webã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
        news_articles = await self.fetcher.fetch_all_news()
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
        print("\\nğŸ—„ï¸ Step 2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ä¸­...")
        new_article_count = 0
        for article in news_articles:
            article_id = self.database.add_news_article(article)
            if article_id:
                new_article_count += 1
                print(f"  âœ… æ–°è¦è¨˜äº‹ç™»éŒ²: {article['title'][:50]}...")
        
        print(f"ğŸ“Š æ–°è¦è¨˜äº‹ç™»éŒ²æ•°: {new_article_count}/{len(news_articles)}")
        
        # 3. æœ€è¿‘ã®è¨˜äº‹ã‚’å–å¾—ã—ã¦çµ„ã¿åˆã‚ã›åˆ†æ
        print("\\nğŸ¤– Step 3: çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ç”Ÿæˆä¸­...")
        recent_articles = self.database.get_recent_articles(20)
        
        if len(recent_articles) >= 2:
            combinations = self.proposer.analyze_news_combinations(recent_articles)
            
            # çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            saved_count = 0
            for combination in combinations:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹IDã‚’å–å¾—
                news1_id = combination['news1'].get('id')
                news2_id = combination['news2'].get('id')
                
                if news1_id and news2_id:
                    proposal_data = {
                        'news1_id': news1_id,
                        'news2_id': news2_id,
                        'common_keywords': combination['common_keywords'],
                        'proposal_text': combination['proposal'],
                        'potential_applications': combination['potential_applications'],
                        'confidence_score': 0.7
                    }
                    
                    proposal_id = self.database.save_combination_proposal(proposal_data)
                    if proposal_id:
                        saved_count += 1
            
            print(f"ğŸ’¾ çµ„ã¿åˆã‚ã›ææ¡ˆä¿å­˜æ•°: {saved_count}")
            
            # çµæœã‚’è¡¨ç¤º
            print("\\nğŸ¯ æœ€æ–°ã®çµ„ã¿åˆã‚ã›ææ¡ˆ:")
            for i, combination in enumerate(combinations[:3], 1):
                print(f"\\n--- ææ¡ˆ {i} ---")
                print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹1: {combination['news1']['title'][:60]}")
                print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹2: {combination['news2']['title'][:60]}")
                print(f"ğŸ”‘ å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(combination['common_keywords'])}")
                print(f"ğŸ’¡ ææ¡ˆ: {combination['proposal']}")
                print(f"ğŸš€ å¿œç”¨å¯èƒ½æ€§:")
                for app in combination['potential_applications'][:3]:
                    print(f"   - {app}")
        else:
            print("âŒ çµ„ã¿åˆã‚ã›åˆ†æã«ååˆ†ãªè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
        
        print("\\nâœ… AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ!")
    
    def show_statistics(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        print("\\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        
        recent_articles = self.database.get_recent_articles(100)
        print(f"  ğŸ“„ ç·è¨˜äº‹æ•°: {len(recent_articles)}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        sources = {}
        for article in recent_articles:
            source = article.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        print("  ğŸ“¡ ã‚½ãƒ¼ã‚¹åˆ¥è¨˜äº‹æ•°:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"    - {source}: {count}ä»¶")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    system = AINewsIntegratedSystem()
    
    print("ğŸ¯ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
    await system.run_full_pipeline()
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    system.show_statistics()

if __name__ == "__main__":
    asyncio.run(main())
'''
    
    with open("ai_news_integrated_system.py", "w", encoding="utf-8") as f:
        f.write(integration_code)
    
    print("âœ… ã™ã¹ã¦ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ")
    
    # 4. ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§ª çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
    
    try:
        import subprocess
        
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ
        test_result = subprocess.run([
            sys.executable, "ai_news_integrated_system.py"
        ], capture_output=True, text=True, cwd=target_project, timeout=30)
        
        if test_result.returncode == 0:
            print("âœ… çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:")
            print(test_result.stdout)
        else:
            print("âŒ çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            print(test_result.stderr)
    
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    print("ğŸ‰ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸ!")

if __name__ == "__main__":
    asyncio.run(execute_integrated_ai_news_system())