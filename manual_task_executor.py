#!/usr/bin/env python3
"""
æ‰‹å‹•ã§AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import json
import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent / "src"))

async def execute_ai_news_combination_system():
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã‚’æ‰‹å‹•å®Ÿè¡Œ"""
    
    print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®æ‰‹å‹•å®Ÿè¡Œã‚’é–‹å§‹...")
    
    # ai-news-digãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹
    target_project = "/Users/tsutomusaito/git/ai-news-dig"
    os.chdir(target_project)
    
    # æœ€æ–°ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’èª­ã¿è¾¼ã¿
    review_session_path = target_project + "/.nocturnal/review_sessions/review_20250920_211203.json"
    
    if not os.path.exists(review_session_path):
        print(f"âŒ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {review_session_path}")
        return
    
    with open(review_session_path, 'r', encoding='utf-8') as f:
        review_data = json.load(f)
    
    print(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯: {review_data['task']['description']}")
    print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {review_data['status']}")
    
    # æ‰¿èªæ¸ˆã¿ã®å ´åˆã¯å®Ÿè¡Œ
    if any(feedback['type'] == 'approve' for feedback in review_data['feedback_history']):
        print("âœ… æ‰¿èªæ¸ˆã¿ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œä¸­...")
        
        # å®Ÿéš›ã®å®Ÿè£…ä½œæ¥­ã‚’é–‹å§‹
        await implement_ai_news_web_fetch_system(target_project, review_data)
        
        print("ğŸ‰ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    else:
        print("âŒ ã‚¿ã‚¹ã‚¯ãŒæ‰¿èªã•ã‚Œã¦ã„ã¾ã›ã‚“")

async def implement_ai_news_web_fetch_system(project_path: str, review_data: dict):
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…"""
    
    print("ğŸ”§ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚’é–‹å§‹...")
    
    # 1. æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèª
    print("ğŸ“ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ç¢ºèªä¸­...")
    
    existing_files = []
    for file in os.listdir(project_path):
        if file.endswith('.py'):
            existing_files.append(file)
            print(f"  âœ… {file}")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®ä½œæˆ
    print("ğŸ—„ï¸ SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‚’ä½œæˆä¸­...")
    
    db_schema = '''import sqlite3
import hashlib
from datetime import datetime
from typing import List, Dict, Optional

class AINewsDatabase:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "ai_news.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
CREATE TABLE IF NOT EXISTS news_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT,
                url TEXT UNIQUE,
                source TEXT,
                published_date TEXT,
                author TEXT,
                keywords TEXT,
                content_hash TEXT UNIQUE,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # çµ„ã¿åˆã‚ã›ææ¡ˆãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
CREATE TABLE IF NOT EXISTS combination_proposals (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                news1_id INTEGER,
                news2_id INTEGER,
                common_keywords TEXT,
                proposal_text TEXT,
                potential_applications TEXT,
                confidence_score REAL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (news1_id) REFERENCES news_articles (id),
                FOREIGN KEY (news2_id) REFERENCES news_articles (id)
            )
        ''')
        
        # ãƒ•ã‚§ãƒƒãƒãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
        cursor.execute('''
CREATE TABLE IF NOT EXISTS fetch_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_type TEXT,
                source_url TEXT,
                fetch_count INTEGER,
                success_count INTEGER,
                error_count INTEGER,
                last_fetch_time TEXT,
                status TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def add_news_article(self, article: Dict) -> Optional[int]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        content_hash = hashlib.md5(
            (article.get('title', '') + article.get('content', '')).encode()
        ).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
INSERT INTO news_articles 
                (title, content, url, source, published_date, author, keywords, content_hash)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.get('title', ''),
                article.get('content', ''),
                article.get('url', ''),
                article.get('source', ''),
                article.get('published_date', ''),
                article.get('author', ''),
                article.get('keywords', ''),
                content_hash
            ))
            
            article_id = cursor.lastrowid
            conn.commit()
            return article_id
            
        except sqlite3.IntegrityError:
            # é‡è¤‡è¨˜äº‹ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            return None
        finally:
            conn.close()
    
    def get_recent_articles(self, limit: int = 50) -> List[Dict]:
        """æœ€è¿‘ã®è¨˜äº‹ã‚’å–å¾—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
SELECT id, title, content, url, source, published_date, author, keywords
            FROM news_articles 
            ORDER BY created_at DESC 
            LIMIT ?
        ''', (limit,))
        
        columns = [description[0] for description in cursor.description]
        articles = []
        
        for row in cursor.fetchall():
            article = dict(zip(columns, row))
            articles.append(article)
        
        conn.close()
        return articles
    
    def save_combination_proposal(self, proposal: Dict) -> int:
        """çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ä¿å­˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
INSERT INTO combination_proposals 
            (news1_id, news2_id, common_keywords, proposal_text, potential_applications, confidence_score)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            proposal.get('news1_id'),
            proposal.get('news2_id'),
            ','.join(proposal.get('common_keywords', [])),
            proposal.get('proposal_text', ''),
            ','.join(proposal.get('potential_applications', [])),
            proposal.get('confidence_score', 0.5)
        ))
        
        proposal_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return proposal_id
'''
    
    db_file = os.path.join(project_path, "ai_news_database.py")
    with open(db_file, 'w', encoding='utf-8') as f:
        f.write(db_schema)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ: {db_file}")
    
    # 3. Webãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã®ä½œæˆ
    print("ğŸŒ Webãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’ä½œæˆä¸­...")
    
    web_fetcher_code = '''import requests
import feedparser
from bs4 import BeautifulSoup
import asyncio
import aiohttp
from datetime import datetime
from typing import List, Dict
import re

class AINewsFetcher:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’Webã‹ã‚‰åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.rss_feeds = [
            'https://feeds.feedburner.com/venturebeat/ai',
            'https://www.artificialintelligence-news.com/feed/',
            'https://machinelearningmastery.com/feed/',
            'https://hai.stanford.edu/news/rss.xml',
            'https://www.deeplearning.ai/feed/'
        ]
        
        self.ai_keywords = [
            'artificial intelligence', 'AI', 'machine learning', 'ML', 'deep learning',
            'neural network', 'GPT', 'ChatGPT', 'LLM', 'natural language processing',
            'computer vision', 'robotics', 'automation', 'algorithm'
        ]
    
    async def fetch_all_news(self) -> List[Dict]:
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        all_news = []
        
        print("ğŸ“¡ RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
        rss_news = await self.fetch_rss_news()
        all_news.extend(rss_news)
        
        print("ğŸ” AIé–¢é€£ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        scraped_news = await self.scrape_ai_news_sites()
        all_news.extend(scraped_news)
        
        print("ğŸ¯ APIã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...")
        api_news = await self.fetch_api_news()
        all_news.extend(api_news)
        
        # é‡è¤‡é™¤å»
        unique_news = self.remove_duplicates(all_news)
        
        print(f"âœ… åˆè¨ˆ {len(unique_news)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¾ã—ãŸ")
        return unique_news
    
    async def fetch_rss_news(self) -> List[Dict]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        news_list = []
        
        for feed_url in self.rss_feeds:
            try:
                print(f"  ğŸ“„ RSSå–å¾—ä¸­: {feed_url}")
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:10]:  # æœ€æ–°10ä»¶
                    if self.is_ai_related(entry.get('title', '') + ' ' + entry.get('summary', '')):
                        news_item = {
                            'title': entry.get('title', ''),
                            'content': entry.get('summary', ''),
                            'url': entry.get('link', ''),
                            'source': feed.feed.get('title', 'RSS Feed'),
                            'published_date': entry.get('published', ''),
                            'author': entry.get('author', ''),
                            'fetch_method': 'RSS'
                        }
                        news_list.append(news_item)
                        
            except Exception as e:
                print(f"    âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({feed_url}): {e}")
        
        return news_list
    
    async def scrape_ai_news_sites(self) -> List[Dict]:
        """AIé–¢é€£ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        news_list = []
        
        # AIé–¢é€£ã‚µã‚¤ãƒˆã®URL
        ai_sites = [
            'https://www.artificialintelligence-news.com/',
            'https://venturebeat.com/ai/',
            'https://www.theverge.com/ai-artificial-intelligence'
        ]
        
        async with aiohttp.ClientSession() as session:
            for site_url in ai_sites:
                try:
                    print(f"  ğŸ•·ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {site_url}")
                    async with session.get(site_url, timeout=10) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆã‚µã‚¤ãƒˆã«ã‚ˆã£ã¦æ§‹é€ ãŒç•°ãªã‚‹ï¼‰
                            articles = soup.find_all(['article', 'div'], class_=re.compile(r'article|post|news'))[:5]
                            
                            for article in articles:
                                title_elem = article.find(['h1', 'h2', 'h3', 'a'])
                                if title_elem:
                                    title = title_elem.get_text(strip=True)
                                    if self.is_ai_related(title):
                                        link = title_elem.get('href') if title_elem.name == 'a' else None
                                        if link and not link.startswith('http'):
                                            link = f"{site_url.rstrip('/')}/{link.lstrip('/')}"
                                        
                                        news_item = {
                                            'title': title,
                                            'content': title,  # è©³ç´°ã¯å¾Œã§å–å¾—å¯èƒ½
                                            'url': link or site_url,
                                            'source': site_url,
                                            'published_date': datetime.now().isoformat(),
                                            'author': '',
                                            'fetch_method': 'Scraping'
                                        }
                                        news_list.append(news_item)
                                        
                except Exception as e:
                    print(f"    âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({site_url}): {e}")
        
        return news_list
    
    async def fetch_api_news(self) -> List[Dict]:
        """News APIãªã©ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        news_list = []
        
        # News API (è¦API Key)
        # api_key = os.getenv('NEWS_API_KEY')
        # if api_key:
        #     # News APIå®Ÿè£…
        #     pass
        
        # GitHub Trending AI projects (ç°¡æ˜“å®Ÿè£…)
        try:
            async with aiohttp.ClientSession() as session:
                url = "https://api.github.com/search/repositories?q=artificial+intelligence&sort=stars&order=desc"
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        for repo in data.get('items', [])[:3]:
                            news_item = {
                                'title': f"AI Project: {repo['name']}",
                                'content': repo.get('description', ''),
                                'url': repo['html_url'],
                                'source': 'GitHub Trending',
                                'published_date': repo.get('created_at', ''),
                                'author': repo.get('owner', {}).get('login', ''),
                                'fetch_method': 'API'
                            }
                            news_list.append(news_item)
        except Exception as e:
            print(f"    âŒ GitHub API ã‚¨ãƒ©ãƒ¼: {e}")
        
        return news_list
    
    def is_ai_related(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒAIé–¢é€£ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in self.ai_keywords)
    
    def remove_duplicates(self, news_list: List[Dict]) -> List[Dict]:
        """é‡è¤‡è¨˜äº‹ã‚’é™¤å»"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title_hash = hash(news.get('title', '').lower().strip())
            if title_hash not in seen_titles:
                seen_titles.add(title_hash)
                unique_news.append(news)
        
        return unique_news
'''
    
    fetcher_file = os.path.join(project_path, "ai_news_fetcher.py")
    with open(fetcher_file, 'w', encoding='utf-8') as f:
        f.write(web_fetcher_code)
    
    print(f"âœ… Webãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼ã‚’ä½œæˆ: {fetcher_file}")
    
    # 4. çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ä½œæˆ
    print("ğŸ”— çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆä¸­...")
    
    integration_code = f'''import asyncio
import sys
import os
from datetime import datetime

# æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ai_news_combination_proposer import AINewsCombinationProposer
from ai_news_database import AINewsDatabase
from ai_news_fetcher import AINewsFetcher

class AINewsIntegratedSystem:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ•ã‚§ãƒƒãƒã€DBç™»éŒ²ã€çµ„ã¿åˆã‚ã›ææ¡ˆã‚’çµ±åˆ"""
    
    def __init__(self):
        self.database = AINewsDatabase()
        self.fetcher = AINewsFetcher()
        self.proposer = AINewsCombinationProposer()
    
    async def run_full_pipeline(self):
        """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹...")
        
        # 1. Webã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
        print("\\nğŸ“¡ Step 1: Webã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
        news_articles = await self.fetcher.fetch_all_news()
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
        print("\\nğŸ—„ï¸ Step 2: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ä¸­...")
        new_article_count = 0
        for article in news_articles:
            article_id = self.database.add_news_article(article)
            if article_id:
                new_article_count += 1
                print(f"  âœ… æ–°è¦è¨˜äº‹ç™»éŒ²: {{article['title'][:50]}}...")
        
        print(f"ğŸ“Š æ–°è¦è¨˜äº‹ç™»éŒ²æ•°: {{new_article_count}}/{{len(news_articles)}}")
        
        # 3. æœ€è¿‘ã®è¨˜äº‹ã‚’å–å¾—ã—ã¦çµ„ã¿åˆã‚ã›åˆ†æ
        print("\\nğŸ¤– Step 3: çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ç”Ÿæˆä¸­...")
        recent_articles = self.database.get_recent_articles(20)
        
        if len(recent_articles) >= 2:
            combinations = self.proposer.analyze_news_combinations(recent_articles)
            
            # çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            saved_count = 0
            for combination in combinations:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹IDã‚’å–å¾—
                news1_id = combination['news1'].get('id')
                news2_id = combination['news2'].get('id')
                
                if news1_id and news2_id:
                    proposal_data = {{
                        'news1_id': news1_id,
                        'news2_id': news2_id,
                        'common_keywords': combination['common_keywords'],
                        'proposal_text': combination['proposal'],
                        'potential_applications': combination['potential_applications'],
                        'confidence_score': 0.7
                    }}
                    
                    proposal_id = self.database.save_combination_proposal(proposal_data)
                    if proposal_id:
                        saved_count += 1
            
            print(f"ğŸ’¾ çµ„ã¿åˆã‚ã›ææ¡ˆä¿å­˜æ•°: {{saved_count}}")
            
            # çµæœã‚’è¡¨ç¤º
            print("\\nğŸ¯ æœ€æ–°ã®çµ„ã¿åˆã‚ã›ææ¡ˆ:")
            for i, combination in enumerate(combinations[:3], 1):
                print(f"\\n--- ææ¡ˆ {{i}} ---")
                print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹1: {{combination['news1']['title'][:60]}}...")
                print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹2: {{combination['news2']['title'][:60]}}...")
                print(f"ğŸ”‘ å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {{', '.join(combination['common_keywords'])}}")
                print(f"ğŸ’¡ ææ¡ˆ: {{combination['proposal']}}")
                print(f"ğŸš€ å¿œç”¨å¯èƒ½æ€§:")
                for app in combination['potential_applications'][:3]:
                    print(f"   - {{app}}")
        else:
            print("âŒ çµ„ã¿åˆã‚ã›åˆ†æã«ååˆ†ãªè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
        
        print("\\nâœ… AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ!")
    
    def show_statistics(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        print("\\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ:")
        
        recent_articles = self.database.get_recent_articles(100)
        print(f"  ğŸ“„ ç·è¨˜äº‹æ•°: {{len(recent_articles)}}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        sources = {{}}
        for article in recent_articles:
            source = article.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        print("  ğŸ“¡ ã‚½ãƒ¼ã‚¹åˆ¥è¨˜äº‹æ•°:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"    - {{source}}: {{count}}ä»¶")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    system = AINewsIntegratedSystem()
    
    print("ğŸ¯ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
    await system.run_full_pipeline()
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    system.show_statistics()

if __name__ == "__main__":
    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèªã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¡ˆå†…
    try:
        import requests
        import feedparser
        import aiohttp
        from bs4 import BeautifulSoup
    except ImportError as e:
        print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {{e}}")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("pip install requests feedparser aiohttp beautifulsoup4")
        sys.exit(1)
    
    asyncio.run(main())
'''
    
    integrated_file = os.path.join(project_path, "ai_news_integrated_system.py")
    with open(integrated_file, 'w', encoding='utf-8') as f:
        f.write(integration_code)
    
    print(f"âœ… çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ: {integrated_file}")
    
    # 5. requirements.txtã®ä½œæˆ
    print("ğŸ“¦ requirements.txtã‚’ä½œæˆä¸­...")
    
    requirements = '''requests>=2.25.1
feedparser>=6.0.8
aiohttp>=3.8.0
beautifulsoup4>=4.9.3
sqlite3
hashlib
'''
    
    requirements_file = os.path.join(project_path, "requirements.txt")
    with open(requirements_file, 'w', encoding='utf-8') as f:
        f.write(requirements.strip())
    
    print(f"âœ… ä¾å­˜é–¢ä¿‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ: {requirements_file}")
    
    # 6. ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
    print("ğŸ§ª çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
    
    try:
        # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        import subprocess
        install_result = subprocess.run([
            sys.executable, "-m", "pip", "install", "requests", "feedparser", "aiohttp", "beautifulsoup4"
        ], capture_output=True, text=True)
        
        if install_result.returncode == 0:
            print("âœ… ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            
            # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ
            test_result = subprocess.run([
                sys.executable, integrated_file
            ], capture_output=True, text=True, cwd=project_path, timeout=60)
            
            if test_result.returncode == 0:
                print("âœ… çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
                print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:")
                print(test_result.stdout[-1000:])  # æœ€å¾Œã®1000æ–‡å­—ã‚’è¡¨ç¤º
            else:
                print("âŒ çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
                print(test_result.stderr)
        else:
            print("âŒ ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            print(install_result.stderr)
    
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {{e}}")
    
    print("ğŸ‰ AIãƒ‹ãƒ¥ãƒ¼ã‚¹Webãƒ•ã‚§ãƒƒãƒãƒ»DBç™»éŒ²ãƒ»çµ„ã¿åˆã‚ã›ææ¡ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸ!")

async def implement_ai_news_combination_system(project_path: str, review_data: dict):
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…"""
    
    print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã‚’é–‹å§‹...")
    
    # 1. æ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèª
    print("ğŸ“ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ç¢ºèªä¸­...")
    
    # ä¸»è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    main_files = [
        "main.py",
        "app.py", 
        "requirements.txt",
        "README.md"
    ]
    
    existing_files = []
    for file in main_files:
        file_path = os.path.join(project_path, file)
        if os.path.exists(file_path):
            existing_files.append(file)
            print(f"  âœ… {file}")
        else:
            print(f"  âŒ {file}")
    
    # 2. AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆæ©Ÿèƒ½ã®å®Ÿè£…
    print("ğŸ¤– AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆæ©Ÿèƒ½ã‚’å®Ÿè£…ä¸­...")
    
    # çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ãƒ¼ãƒ‰
    combination_system_code = '''
# AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ 
class AINewsCombinationProposer:
    """AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã®çµ„ã¿åˆã‚ã›ã‚’åˆ†æã—ã€æ–°ã—ã„å¯èƒ½æ€§ã‚’ææ¡ˆã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.news_database = []
        self.combination_patterns = []
    
    def analyze_news_combinations(self, news_list):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®çµ„ã¿åˆã‚ã›ã‚’åˆ†æ"""
        combinations = []
        
        for i, news1 in enumerate(news_list):
            for j, news2 in enumerate(news_list[i+1:], i+1):
                combination = self.create_combination_proposal(news1, news2)
                if combination:
                    combinations.append(combination)
        
        return combinations
    
    def create_combination_proposal(self, news1, news2):
        """2ã¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ç”Ÿæˆ"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords1 = self.extract_keywords(news1)
        keywords2 = self.extract_keywords(news2)
        
        # å…±é€šè¦ç´ ã®ç™ºè¦‹
        common_elements = set(keywords1) & set(keywords2)
        
        if common_elements:
            return {
                'news1': news1,
                'news2': news2,
                'common_keywords': list(common_elements),
                'proposal': self.generate_proposal_text(news1, news2, common_elements),
                'potential_applications': self.suggest_applications(news1, news2)
            }
        
        return None
    
    def extract_keywords(self, news_item):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # ç°¡å˜ãªå®Ÿè£…ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šé«˜åº¦ãªNLPå‡¦ç†ãŒå¿…è¦ï¼‰
        text = news_item.get('title', '') + ' ' + news_item.get('content', '')
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        ai_keywords = ['AI', 'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ML', 'LLM', 'GPT', 'ChatGPT', 
                      'æ·±å±¤å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'è‡ªç„¶è¨€èªå‡¦ç†', 'NLP']
        
        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        tech_keywords = ['API', 'ã‚¯ãƒ©ã‚¦ãƒ‰', 'IoT', 'ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³', 'AR', 'VR', 
                        '5G', 'ã‚¨ãƒƒã‚¸ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°', 'ã‚³ãƒ³ãƒ†ãƒŠ', 'Kubernetes']
        
        # ç”£æ¥­ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        industry_keywords = ['è‡ªå‹•è»Š', 'åŒ»ç™‚', 'é‡‘è', 'æ•™è‚²', 'è£½é€ æ¥­', 'è¾²æ¥­', 
                           'ç‰©æµ', 'å°å£²', 'ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ', 'ã‚²ãƒ¼ãƒ ']
        
        all_keywords = ai_keywords + tech_keywords + industry_keywords
        
        found_keywords = []
        for keyword in all_keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def generate_proposal_text(self, news1, news2, common_elements):
        """çµ„ã¿åˆã‚ã›ææ¡ˆãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        return f"ã€Œ{news1.get('title', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹1')}ã€ã¨ã€Œ{news2.get('title', 'ãƒ‹ãƒ¥ãƒ¼ã‚¹2')}ã€ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€{', '.join(common_elements)}ã®åˆ†é‡ã§æ–°ã—ã„å¯èƒ½æ€§ãŒç”Ÿã¾ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
    
    def suggest_applications(self, news1, news2):
        """å…·ä½“çš„ãªå¿œç”¨ä¾‹ã‚’ææ¡ˆ"""
        applications = [
            "æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®æ©Ÿèƒ½æ‹¡å¼µ",
            "æ–°ã—ã„ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‰µå‡º", 
            "æŠ€è¡“çš„èª²é¡Œã®è§£æ±ºç­–",
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å‘ä¸Š",
            "ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®å®Ÿç¾"
        ]
        
        return applications[:3]  # ä¸Šä½3ã¤ã‚’è¿”ã™
'''
    
    # çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    combination_file = os.path.join(project_path, "ai_news_combination_proposer.py")
    with open(combination_file, 'w', encoding='utf-8') as f:
        f.write(combination_system_code)
    
    print(f"âœ… çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ: {combination_file}")
    
    # 3. çµ±åˆã‚³ãƒ¼ãƒ‰ã®ä½œæˆ
    print("ğŸ”— æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆä¸­...")
    
    integration_code = '''
# AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
from ai_news_combination_proposer import AINewsCombinationProposer

def integrate_combination_system():
    """æ—¢å­˜ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿åˆã‚ã›ææ¡ˆæ©Ÿèƒ½ã‚’çµ±åˆ"""
    
    print("ğŸ¤– AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
    proposer = AINewsCombinationProposer()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã¯æ—¢å­˜ã®DBã‹ã‚‰å–å¾—ï¼‰
    sample_news = [
        {
            'title': 'ChatGPT APIã‚’æ´»ç”¨ã—ãŸæ–°ã—ã„æ•™è‚²ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ',
            'content': 'äººå·¥çŸ¥èƒ½ã‚’æ´»ç”¨ã—ãŸå€‹åˆ¥å­¦ç¿’æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ãŒé–‹ç™ºã•ã‚ŒãŸ',
            'source': 'AI Education News'
        },
        {
            'title': 'IoTã‚»ãƒ³ã‚µãƒ¼ã‚’ä½¿ã£ãŸè¾²æ¥­ã®è‡ªå‹•åŒ–',
            'content': 'IoTæŠ€è¡“ã¨AIã‚’çµ„ã¿åˆã‚ã›ãŸã‚¹ãƒãƒ¼ãƒˆè¾²æ¥­ãŒæ™®åŠä¸­',
            'source': 'Smart Agriculture Today'
        },
        {
            'title': 'åŒ»ç™‚ç¾å ´ã§ã®AIè¨ºæ–­æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ',
            'content': 'æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ãŸç”»åƒè¨ºæ–­ã®ç²¾åº¦ãŒå‘ä¸Š',
            'source': 'Medical AI Report'
        }
    ]
    
    print("ğŸ“Š ãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›åˆ†æã‚’å®Ÿè¡Œä¸­...")
    combinations = proposer.analyze_news_combinations(sample_news)
    
    print(f"ğŸ¯ {len(combinations)}ä»¶ã®çµ„ã¿åˆã‚ã›ææ¡ˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ:")
    
    for i, combination in enumerate(combinations, 1):
        print(f"\\n--- ææ¡ˆ {i} ---")
        print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹1: {combination['news1']['title']}")
        print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹2: {combination['news2']['title']}")
        print(f"ğŸ”‘ å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(combination['common_keywords'])}")
        print(f"ğŸ’¡ ææ¡ˆ: {combination['proposal']}")
        print(f"ğŸš€ å¿œç”¨å¯èƒ½æ€§:")
        for app in combination['potential_applications']:
            print(f"   - {app}")
    
    return combinations

if __name__ == "__main__":
    integrate_combination_system()
'''
    
    integration_file = os.path.join(project_path, "integration_demo.py")
    with open(integration_file, 'w', encoding='utf-8') as f:
        f.write(integration_code)
    
    print(f"âœ… çµ±åˆãƒ‡ãƒ¢ã‚’ä½œæˆ: {integration_file}")
    
    # 4. å®Ÿéš›ã«å®Ÿè¡Œã—ã¦ãƒ†ã‚¹ãƒˆ
    print("ğŸ§ª ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
    
    try:
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œ
        import subprocess
        result = subprocess.run([sys.executable, integration_file], 
                              capture_output=True, text=True, cwd=project_path)
        
        if result.returncode == 0:
            print("âœ… ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:")
            print(result.stdout)
        else:
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            print(result.stderr)
    
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    print("ğŸ‰ AIãƒ‹ãƒ¥ãƒ¼ã‚¹çµ„ã¿åˆã‚ã›ææ¡ˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸ!")

if __name__ == "__main__":
    asyncio.run(execute_ai_news_combination_system())